# CharLM-MLP ğŸš€
*A character-level language model from scratch, building towards GPT-like architectures.*

## ğŸ“– Overview
This project implements a **character-level language model** trained on a dataset of baby names. It follows the â€œZero to Heroâ€ progression (bigram â†’ MLP â†’ transformer), but all code is written **from scratch in PyTorch**, without using high-level modules like `nn.Linear` or `nn.Sequential`.

The goal is to deeply understand how embeddings, hidden layers, normalization, and training dynamics come together to generate text.

---

## ğŸ”„ Evolution of the Model

### **1. Initial MLP Model**
- Context size = 3 characters.  
- Embeddings â†’ hidden layer (tanh) â†’ logits for next character.  
- Trained with cross-entropy loss.  
- Generated names, but training was unstable and loss curves were noisy.  

### **2. Improved Model with Batch Normalization**
- Added **manual BatchNorm** between the hidden pre-activations and `tanh`.  
- Purpose: stabilize distributions, avoid saturation, improve gradient flow.  
- Implemented running mean/std for inference mode.  
- Result:  
  - Faster convergence.  
  - Smoother loss curves.  
  - More coherent sampled names.  

---

## ğŸ“Š Results

**Training Loss Curve:**  
*![Activation Distribution](image-2.png)*  

*![Gradient Distribution](image-3.png)*

*![Weights Gradient Distribution](image-4.png)*

**Generated Sample Names:**  
- `Loma`  
- `Deren`  
- `Mariel`  
- â€¦  

---

## âš™ï¸ How It Works
1. **Data prep**:  
   - Reads `names.txt`.  
   - Builds vocabulary + integer mappings.  
   - Creates `(context, next_char)` training pairs.  

2. **Model**:  
   - Embedding matrix for characters.  
   - Flattened embedding â†’ linear layer â†’ **BatchNorm â†’ tanh** â†’ output layer.  
   - Cross-entropy loss.  

3. **Training loop**:  
   - Manual forward/backward passes.  
   - Mini-batch SGD.  
   - Plots loss for training/dev sets.  

4. **Sampling**:  
   - Generates new names one char at a time until `.`.  

---

## ğŸ§  Key Learnings
- How embeddings compress character information into dense vectors.  
- Why **BatchNorm** stabilizes training and reduces internal covariate shift.  
- How manual backpropagation deepens understanding of PyTorch internals.  
- The importance of architectural tweaks for training stability.  

---

## ğŸš€ Next Steps
- Add **Dropout** for regularization.  
- Train on a **larger, real-world dataset**.  
- Extend to **multi-layer MLP** or a **Transformer block**.  
- Compare **with vs without BatchNorm** quantitatively.  

---

## ğŸ“‚ Repo Structure
```
CharLM-MLP/
â”‚â”€â”€ CharLM_MLP_2.ipynb   # Notebook with BatchNorm implementation
â”‚â”€â”€ names.txt            # Baby names dataset
â”‚â”€â”€ README.md            # Project documentation
```

---

## ğŸ”— Credits
- Inspired by [Andrej Karpathyâ€™s Zero to Hero series](https://github.com/karpathy/nn-zero-to-hero).  
- All improvements (BatchNorm implementation, experiments, analysis) coded and documented by **Harsha Vardhan**.  
